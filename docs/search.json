[
  {
    "objectID": "Final-Review.html",
    "href": "Final-Review.html",
    "title": "Final Review Sheet",
    "section": "",
    "text": "We made it!\nI am quite proud of all of you for making it to this point. This was never an easy class, and it should not be. This class is designed to show you the basic concepts that underlie all of psychological analyses. Whether you go on to be clinical psychologists or in an entirely unrelated field, I hope that you will take away an understanding, and an appreciation for the science that floods the news daily. Being able to understand what all those numbers mean will make you a better consumer of scientific knowledge.\n\n\nWe have learned three different kinds of ANOVA in this class.\n\n\naov(DV~IV)\n\nDV is what is being measured, usually a score, rating, measurement, etc.\nIV is what is being manipulated, in order for R to calculate anything, this must be a factor.\nfactor(variable) - This will produce the variable as a factor.\nThis produces One F\n\n\n\n\naov(DV~IV1*IV2)\n\nDV is what is being measured, usually a score, rating, measurement, etc.\nIV1 is what is being manipulated: Control, Treatment, Treatment 2, etc.\nIV2 is also what is being manipulated throughout the IV1.\n\nControl-High, Control-Medium, Control-Low\n\nThis produces Three F’s\n\n\n\n\naov(DV~IV+Error(Subject))\n\nDV is what is being measured, usually a score, rating, measurement, etc.\nIV1 is what is being manipulated: Control, Treatment, Treatment 2, etc.\nSubject is the ID of the participant taking the experiment, they factor into your analysis because their data is in each level of the independent variable. Must be a factor.\nThis produces One F.\n\n\n\n\n\n\n\nwilcox.test(y~A)\n\nwhere y is numeric\nA is A binary factor (0,1) | (yes, no)\n\n\n\n\nwilcox.test(y,x) #\n\ny is numeric x is numeric\n\n\n\n\nwilcox.test(y1,y2,paired=TRUE)\n\nWhere y1 and y2 are numeric\n\n\n\n\nkruskal.test(y~A)\n\nWhere y1 is numeric and A is a factor\n\n\n\n\nfriedman.test(y~A|B)\n\ny is data values\nA is a grouping factor\nB is a blocking factor"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 3400: Statistics",
    "section": "",
    "text": "Understanding Statistics in the Behavioral Sciences by Robert Pagano\n\n\n\nR For Dummies 2nd Edition\n\n\n\nDavid Brocker\nDavidABrocker@gmail.com\nOffice Hours: 12:00-1:30 Tuesday’s in 4606A or by Appointment\n\n\n\nThe lab section serves as a way to go from the theoretical and conceptual materials and transform them into the practical applications that they serve. In lecture, you will learn about why a certain statistic is calculated when dealing with specific data, in lab, you will learn how.\nAdditionally, the labs will serve as a way to review the material on the exercises. I will give you problems with different numbers, but they will test the same type of knowledge that the exercises aim to test.\nIf you miss a lab, it is quite difficult to catch up. I will do my best to make my notes available, but there is a difference in reading it versus seeing it first hand.\nDuring lab you will be shown how to do various parts of the exercises using R-Studio. This is the best time for you to ask me any questions you have."
  },
  {
    "objectID": "Lab-1.html",
    "href": "Lab-1.html",
    "title": "Lab 1",
    "section": "",
    "text": "Introduction\n\nHello!\nWelcome to PSYC 3400: Statistical Methods in Behvioral Research. In this course, we will be using R-Studio in order to visualize and implement what you learn in lecture.\nIn other statistics classes, students will be learning the same exact materials using a program called SPSS. The main difference between SPSS and R-Studio is usability and price. SPSS is a proprietary product, which means it costs a lot of money. R-Studio on the other hand is freeware–which, as the name suggests is free.\nR-Studio\nR-Studio, or R as we will refer to it going forwards, is by no means difficult, however, it does require that you learn a new way to think. It can be downloaded here\nSimple Math Operations\nR can do simple math operations: (Addition, Subtraction, Division, Multiplication, Exponentiation)\n\n# Addition\n2 + 2\n\n[1] 4\n\n# Subtraction\n4 - 1\n\n[1] 3\n\n# Division\n10 / 2\n\n[1] 5\n\n# Multiplication\n3 * 3\n\n[1] 9\n\n# Exponentiation \n6^2\n\n[1] 36\n\n\nThose same operations can be saved to a variable. Variables will hold that value when they are ‘called’ later.\n\nx = 2 + 2\nx\n\n[1] 4\n\n\nR follows the order of operations (PEMDAS)\n\ny = (2+2)+(10/5)+(2*3)\ny\n\n[1] 12\n\n\nVariables\nWe can take a value and assign it to a variable. We can also take two variables and perform math operations on them. When dealing with real data, it is important to assign understandable variable names.\n\nx = 2\ny = 2\nx + y\n\n[1] 4\n\nmean_age = 26\n\nIn this class, we will often use data sets. Single numbers are important, but most, if not all tests in Psychology make use of a dataset consisting of several numbers. In order to make a dataset in R we have two options.\n\nWe can upload a file to R and it will import accordingly\nWe can manually create the dataset, using a function called concatenation.\n\n\nx = c(1,2,3,4)\nx\n\n[1] 1 2 3 4\n\nages = c(20,18,19,21,22,22,18,19,20,20,21)\nages\n\n [1] 20 18 19 21 22 22 18 19 20 20 21\n\n\nCentral Tendency\n\nMeasures of Central Tendency\n\nMean - returns the average of the sample\nMedian = returns the middle number of the sample when put in ascending order\nRange = returns the lowest and highest data points in the set\n\n\n\n\nx = c(1,3,5,7,9,11)\nmean(x)\n\n[1] 6\n\nmedian(x)\n\n[1] 6\n\nrange(x)\n\n[1]  1 11\n\n\nThe mean and median tell us something about the data. They tell us that in this case, the mean for x is 6 and the median of x is 6. When the median and the mean are the same it leads us to believe this is a fairly symmetrical dataset. In order to test this, we can plot a histogram. A histogram shows us frequency counts for every data point.\n\nx = c(1,3,5,7,9,11)\nhist(x)\n\n\n\n\nBetween 0-1 there is exactly 1 point, between 2-4 there is exactly 1 point, etc. Let’s see what it looks like when there is some variation in the data.\n\nx = c(1,3,2,5,7,8,6,3,4,4,5,5,5,5,2,1,6,8,10)\nhist(x)\n\n\n\n\nIn this course, the data that we are dealing with will have different properties assigned to it. Doing this will allow you to attach meaning into your interpretation while also showing you how to not only visualize, but also organize the data. We will be adding two new functions here: main = “” and xlab = ““. The main function tells R what to label the plot, and the xlab function tells R what to label the x-axis.\nConsider the following:\n\nA psychologist is interested in whether or not the students in his class are Android users or iPhone users. More specifically, he is interested in whether or not students who use Android phones spend more time on it during class than iPhone users. He observes his statistics class and obtains the following data.\n\n\nAndroid_Use = c(1,1,1,1,2,2,2,1,1,1,2,3,2,1)\niPhone_Use = c(1,1,1,0,0,0,1,1,1,0,0,0,1,1)\nhist(Android_Use, main = \"Android Use\", xlab = \"Phone Use in Minutes\")\n\n\n\nhist(iPhone_Use, main = \"iPhone Use\", xlab = \"Phone Use in Minutes\")\n\n\n\n\nSo far we have learned that R allows us to do simple math problems, create datasets, extract some descriptions from the dataset and visualize how the data is distributed.\nThe next step is to look at data and see if there are any relationships present. Most of the data you collect or are given will have an established relationship. To start, let’s take a look at a dataset that is manufactured to have a somewhat perfect relationship.\n\nx = c(1,2,3,4)\ny = c(2,4,6,8)\nplot(x~y)\n\n\n\n\nWe can clearly see that the distance between each point is the same. Without any idea of what x and y represent all we can say is that it appears that there seems to be a relationship or a pattern.\n\nx = c(1,2,3,4)\ny = c(2,4,6,8)\nplot(x~y, main = \"Pay Earned and Hours Worked\", xlab = \"Hours Worked\", ylab = \"Amount Paid\")\n\n\n\n\nRealistically, looking at this makes sense but are we to belive that working 4 hours yields a result of $2 pay? The data you look at should have context attached to it. Additionally, most data that you are working with will have more than 4 data points.\nSo let’s see what a dataset of 20 does to our visualiztion.\n\nHours_Slept = round(rnorm(20,6.3),digits = 2)\nHours_Watched_TV = round(rnorm(20,4),digits = 2)\nplot(Hours_Slept~Hours_Watched_TV, main = \"Hours of TV Watched vs. Hours Slept\", xlab = \"Hours Slept\", ylab =\"Hours of TV Watched\")\n\n\n\n\nThe function used in the example above, rnorm is not vital for you to know at this point, but I want to make sure that anything I do, you can see so you understand where information is coming from.\nFrom this data set, we wouldn’t really be able to make any discernible, objective interpretation of this plot.\nIs there a relationship between hours of TV watched and hours slept? Maybe, but not in the data we collected! This is the important thing to note about this class, and most of science that you will see in your life. Just because you see a chart or a graph, does not mean that it is right or true. I am here to teach you how to use R-Studio to perform statistical calculations, but I am also here to be a proponent for scientific literacy.\nDataframes\nMost of the data you will be dealing with in this class will be from a set. The examples in the beginning of class focused mainly on simple operations in R. A “real” dataset may have several variables.\n\nage = c(10,13,13,15,14,13,12,14,13,15,11,12)\ntest_score = c(50,77,85,90,64,87,90,100,83,85,81,70)\nabsences = c(0,0,1,3,2,4,3,1,0,0,0,1)\nage_experiment <-data.frame(age,test_score,absences)\nage_experiment\n\n   age test_score absences\n1   10         50        0\n2   13         77        0\n3   13         85        1\n4   15         90        3\n5   14         64        2\n6   13         87        4\n7   12         90        3\n8   14        100        1\n9   13         83        0\n10  15         85        0\n11  11         81        0\n12  12         70        1\n\n\nIn this hypothetical, a researcher is interested in whether or not the number of absences a student has, has any effect on the grade that student receives on a test. In order to test this, we should probably make a plot of the different variables. We could just type out the variables, but it is better to “call” them using a $.\n\nplot(\n  # Graph Absences on the x-axis and Grade on the y-axis\n  age_experiment$test_score~age_experiment$absences,\n  # Give the plot a  title\n  main = \"Absences vs. Test Scores\", \n  # Give each axis a label\n  xlab = \"Student Absences\",      \n  ylab = \"Student Test Scores\")"
  },
  {
    "objectID": "Lab-10.html",
    "href": "Lab-10.html",
    "title": "Lab 10",
    "section": "",
    "text": "ANOVA’s are used when the sample data consists of a dependent measure and more than two independent variables.\nThe sample data will have a dependent variable, and a grouping variable (IV). This grouping variable needs to be made into a factor.\nIn order to do this, we need to use the factor() function.\nWhen R imports a csv file, there is an option to automatically import any string data as a factor.\nComputing the ANOVA in R is very similar to computing a linear regression:\n\n# For reproduction\nset.seed(122)\n\n# Create group vector\ngroups <-rep(c(\"ctrl\",\"trt1\",\"trt2\",\"trt3\"),each=10)\n\n# Create dv vector\ndv <-c(rnorm(10,3,5),rnorm(10,9,5),rnorm(10,18,5),rnorm(10,36,5))\n\n# Create dataframe\naovex <-data.frame(groups,dv)\n\n# Compare output from aov and lm\nsummary(aov(dv~groups,data=aovex))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ngroups       3   6909  2303.0    71.3 3.24e-15 ***\nResiduals   36   1163    32.3                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm(dv~groups,data=aovex))\n\n\nCall:\nlm(formula = dv ~ groups, data = aovex)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2436  -3.4215   0.4346   4.8907   9.0555 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    3.072      1.797   1.709   0.0960 .  \ngroupstrt1     4.523      2.542   1.779   0.0836 .  \ngroupstrt2    15.634      2.542   6.151 4.37e-07 ***\ngroupstrt3    34.080      2.542  13.409 1.44e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.683 on 36 degrees of freedom\nMultiple R-squared:  0.8559,    Adjusted R-squared:  0.8439 \nF-statistic:  71.3 on 3 and 36 DF,  p-value: 3.24e-15\n\n\nYou should notice that the two outputs share certain values. The F statistic is the same in both cases, as is the p-value. Additionally, the linear regression has the same two degrees of freedom: 3 and 36.\nWhen you are creating your ANOVA output, it is good practice to name the ANOVA as being a something.mod. This is helpful because you will be able to use this model name in other instances.\n\nPost-Hoc tests are useful because they allow us to see where the difference is in our data. If we have a One-Way ANOVA with five groups and we have a significant F value, we have no idea which groups are different.\nWe will be using two Post-Hoc tests: the TukeyHSD and the SNK.Test (Student Newman Keuls).\nWe will use the same data from before and try to find out where the difference is, or rather, which combinations of data are significantly different.\n\nThe TukeyHSD test stands for “Honestly-Significantly-Different”.\n\n# Create model\naovex.mod <-aov(dv~groups, data = aovex)\n\n# Calculate post-hoc differences\nTukeyHSD(aovex.mod)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = dv ~ groups, data = aovex)\n\n$groups\n               diff       lwr      upr     p adj\ntrt1-ctrl  4.522817 -2.322353 11.36799 0.2995619\ntrt2-ctrl 15.633945  8.788776 22.47911 0.0000025\ntrt3-ctrl 34.079840 27.234671 40.92501 0.0000000\ntrt2-trt1 11.111128  4.265959 17.95630 0.0005603\ntrt3-trt1 29.557023 22.711854 36.40219 0.0000000\ntrt3-trt2 18.445895 11.600726 25.29106 0.0000001\n\n\nFrom this output we can see that the p adj value corresponds to whether or not the specific comparison is significant.\nWe can see that there is a difference between the following groups:\n\nTreatment 2 and Control\nTreatment 3 and Control\nTreatment 2 and Treatment 1\nTreatment 3 and Treatment 1\nTreatment 3 and Treatment 2.\n\nThere is not a significant difference between Treatment 1 and Control.\nIf you look at the columns labeled lwr and upr, these are the confidence intervals for this specific comparison. If the confidence intervals include 0, the comparison will not be significant.\nTo test this, if you notice, the comparison between Treatment and Control is the only comparison with a negative lower interval. The range of -2.32 -> 11.36 includes 0 so we can determine it is not significant.\nThese results can also be visualized in the form of a plot.\n\nIn order to plot the Tukey Test results you will need to pass the output of the Tukey Test into the plot() function.\n\n# Save the test to a variable\ntukey.test<-TukeyHSD(aovex.mod)\n\n# Plot the test\nplot(tukey.test)\n\n\n\n\nAs you can see from the graph, there are dashed lines to demarcate the line at 0. The only interval that crosses this range is the Treatment 1 and Control.\n\nThe Student Newman Keuls test is another post-hoc test that is used to make comparisons of differences between groups.\nThe Student Newman Keuls test is available through a package called agricolae.\nFirst, we will install the package using install.packages(\"PackageName\").\nSecond, we will load the package through the use of library(agricolae).\nThe arguments in the Student Newman Keuls test are as follows:\n\ny - This is where you will put your model, (this is why it is so important to store your models!)\ntrt - This is the name of your grouping variable, it will need to appear in quotes.\nmain - This is what the title of your output will be.\nalpha - This is where you will specify what alpha level you will be testing your comparisons against.\nconsole - This is the option that asks whether you want to see the output (you should always make it equal to TRUE)\n\nIn full:\n\n# Load in the package\nlibrary(agricolae)\n\nWarning: package 'agricolae' was built under R version 4.2.3\n\n# Conduct the SNK\nSNK.test(aovex.mod, \n         trt = \"groups\", \n         main = \"Test Student Newman Keuls\", \n         alpha = .05,\n         console = TRUE,\n         DFerror = aovex.mod$df.residual,\n         MSerror = 32.3)\n\n\nStudy: Test Student Newman Keuls\n\nStudent Newman Keuls Test\nfor dv \n\nMean Square Error:  32.29925 \n\ngroups,  means\n\n            dv      std  r       Min      Max\nctrl  3.071869 5.106199 10 -6.010284 10.24372\ntrt1  7.594686 6.393097 10 -3.312952 14.79588\ntrt2 18.705814 5.334151 10 12.608462 27.76132\ntrt3 37.151709 5.813679 10 25.908129 44.00919\n\nAlpha: 0.05 ; DF Error: 36 \n\nCritical Range\n       2        3        4 \n5.154651 6.212483 6.845169 \n\nMeans with the same letter are not significantly different.\n\n            dv groups\ntrt3 37.151709      a\ntrt2 18.705814      b\ntrt1  7.594686      c\nctrl  3.071869      c\n\n\nThe output from the Newman Keuls test does not return p-values, however, the same ideas about the confidence intervals apply.\nYou will notice that Control and Treatment have ranges that include 0, (-6.01-10) and (-3.31-14.79) respectively.\nThe output also prints out a helpful tool: Means with the same letter are not significantly different.\nWe can see that Treatment 3 and Treatment 2 are both different and Treatment 1 and Control are the same letter, indicating they are not significantly different from each other."
  },
  {
    "objectID": "Lab-11.html",
    "href": "Lab-11.html",
    "title": "Lab 11",
    "section": "",
    "text": "Recall the way that the ANOVA is formatted:\n\n# Create the IV\ngroups = rep(c(\"Control\",\"Treatment 1\", \"Treatment 2\"), each = 20)\n\n# Create the DV\nscore =c(rnorm(20,10),rnorm(20,15),rnorm(20,22))\n\n# Combine\nonefact <- data.frame(groups,score)\n\nWe have three independent variables, or conditions, control, treatment 1 and treatment 2. We have one dependent variable, some idea of “score”.\nThe ANOVA is analyzed through the use of the aov function. Remember to save the analysis as a model so you can use it later if you need to do any post-hoc tests/unplanned comparisons.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \ngroups       2 1558.6   779.3   802.7 <2e-16 ***\nResiduals   57   55.3     1.0                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis produces:\n\na sum of squares and mean squares for the between subjects factor, groups.\nIt also produces a sum of squares and mean squares for the within subjects factor, also called the residuals, or the error.\nFrom this we get one F-value.\n\n\nThe current design will not help us too much so we need to move on to another design, the two factor between subjects ANOVA.\nThe set-up is mostly the same:\n\n# Create sex variable \nSex = rep(c(\"Male\",\"Female\"),each = 30)\n\nDiet = rep(c(\"Diet 1\", \"Diet 2\"), 30)\n\nCount = c(rnorm(30,28,2),rnorm(30,35,2))\n\ntwofact.df <- data.frame(Sex,Diet,Count)\n\ntwofact.df\n\n      Sex   Diet    Count\n1    Male Diet 1 27.21052\n2    Male Diet 2 32.84692\n3    Male Diet 1 29.21859\n4    Male Diet 2 21.97352\n5    Male Diet 1 28.89591\n6    Male Diet 2 30.40958\n7    Male Diet 1 27.93257\n8    Male Diet 2 29.97841\n9    Male Diet 1 26.99841\n10   Male Diet 2 28.34294\n11   Male Diet 1 28.84722\n12   Male Diet 2 32.02985\n13   Male Diet 1 30.11101\n14   Male Diet 2 25.47370\n15   Male Diet 1 26.47701\n16   Male Diet 2 26.59372\n17   Male Diet 1 31.19911\n18   Male Diet 2 24.85055\n19   Male Diet 1 26.67166\n20   Male Diet 2 26.10828\n21   Male Diet 1 23.96153\n22   Male Diet 2 27.52581\n23   Male Diet 1 30.43324\n24   Male Diet 2 30.21951\n25   Male Diet 1 24.89281\n26   Male Diet 2 26.58385\n27   Male Diet 1 24.50550\n28   Male Diet 2 28.16369\n29   Male Diet 1 26.89186\n30   Male Diet 2 25.85160\n31 Female Diet 1 37.97109\n32 Female Diet 2 35.06345\n33 Female Diet 1 37.08214\n34 Female Diet 2 36.37917\n35 Female Diet 1 36.44966\n36 Female Diet 2 33.37515\n37 Female Diet 1 37.22250\n38 Female Diet 2 35.95934\n39 Female Diet 1 33.68895\n40 Female Diet 2 37.92897\n41 Female Diet 1 35.34230\n42 Female Diet 2 34.36147\n43 Female Diet 1 33.65918\n44 Female Diet 2 29.95844\n45 Female Diet 1 31.24312\n46 Female Diet 2 37.03697\n47 Female Diet 1 33.39423\n48 Female Diet 2 37.23043\n49 Female Diet 1 35.48737\n50 Female Diet 2 34.82487\n51 Female Diet 1 33.01039\n52 Female Diet 2 34.51271\n53 Female Diet 1 34.42723\n54 Female Diet 2 32.09866\n55 Female Diet 1 36.89583\n56 Female Diet 2 39.32543\n57 Female Diet 1 31.69242\n58 Female Diet 2 36.67778\n59 Female Diet 1 36.30411\n60 Female Diet 2 31.19938\n\n\nWe can see that there are two levels of the independent variable Sex and two levels of the independent variable Diet and one dependent variable, count.\nFrom this we will generate four sums of squares and four mean squares.\n\nSum of Squares for Factor A\nSum of Squares for Factor B\nSum of Squares for the interaction between A and B\nSum of Squares within, or Error, or residual.\n\nAnalyzing the data just uses one more term:\n\n# Create the model\ntwofact.mod<-aov(Count~Sex*Diet,data = twofact.df)\n\n# Calculate summary statistics\nsummary(twofact.mod)\n\n            Df Sum Sq Mean Sq F value  Pr(>F)    \nSex          1  796.5   796.5 131.085 2.7e-16 ***\nDiet         1    0.4     0.4   0.062   0.804    \nSex:Diet     1    0.0     0.0   0.001   0.973    \nResiduals   56  340.2     6.1                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOkay, so we have the anova table, but what do these results look like? From this data, we have three possibilities:\n\nThere is a main effect of Sex\nThere is a main effect of Diet\nThere is an interaction on Sex and Diet\n\nIn order to plot these relationships we use interaction.plot.\n\n# Plot the interaction\ninteraction.plot(twofact.df$Sex,twofact.df$Diet, twofact.df$Count,\n                 type = \"o\",\n                 legend = TRUE,\n                 xlab = \"Sex\",\n                 ylab = \"Mean of Count\",\n                 trace.label = \"Diet Type\")\n\n\n\n\n\n\n# Create DV with variable means (15,20,30,40)\nscored=rnorm(80,c(15,20,15,20))\n\n# Create IV with two levels\niv1 = rep(c(\"Level 1\",\"Level 2\"),each=2,20)\n\n# Create IV with two levels\niv2 = rep(c(\"Group 1\",\"Group 2\"),each=1,40)\n\n# Combine\ndf <- data.frame(iv1,iv2,scored)\n\n# Create Model and plot\nsummary(aov(scored~iv1*iv2,data = df))\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \niv1          1    0.2     0.2   0.148  0.702    \niv2          1  469.2   469.2 435.549 <2e-16 ***\niv1:iv2      1    0.3     0.3   0.311  0.579    \nResiduals   76   81.9     1.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ninteraction.plot(iv1,iv2,scored)"
  },
  {
    "objectID": "Lab-11.html#two-factor-within-subjects-anova",
    "href": "Lab-11.html#two-factor-within-subjects-anova",
    "title": "Lab 11",
    "section": "Two Factor Within Subjects ANOVA",
    "text": "Two Factor Within Subjects ANOVA\nIn a Two Factor Between Subjects ANOVA, a particular participant is only ever in one condition or group or treatment level.\nIn a Two Factor Within Subjects ANOVA, a particular participant is in each condition or group or treatment level.\nSetting this data up uses the same principles as we have learned before.\nThe one major difference in the set-up of the data is that there is now a variable of the subject itself.\nWhen we had a between subjects design, each participant was unique, with a within design, each participant experiences every aspect of the experiment so it is reasonable that they may have an effect on the experiment itself.\nHere is a sample dataset:\n\n# For reproduction\nset.seed(1234)\n\n# Create subject vector\nsubject <- rep(1:10,4)\n\n# Create IV\ngroups <- rep(c(\"pre\",\"post\",\"post 1\", \"post 2\"),each = 10)\n\n# Create DV\nscore <- rnorm(40,c(20,30,40,70))\n\n# Create dataframe\nwithin_anova <- data.frame(subject,groups,score)\nwithin_anova\n\n   subject groups    score\n1        1    pre 18.79293\n2        2    pre 30.27743\n3        3    pre 41.08444\n4        4    pre 67.65430\n5        5    pre 20.42912\n6        6    pre 30.50606\n7        7    pre 39.42526\n8        8    pre 69.45337\n9        9    pre 19.43555\n10      10    pre 29.10996\n11       1   post 39.52281\n12       2   post 69.00161\n13       3   post 19.22375\n14       4   post 30.06446\n15       5   post 40.95949\n16       6   post 69.88971\n17       7   post 19.48899\n18       8   post 29.08880\n19       9   post 39.16283\n20      10   post 72.41584\n21       1 post 1 20.13409\n22       2 post 1 29.50931\n23       3 post 1 39.55945\n24       4 post 1 70.45959\n25       5 post 1 19.30628\n26       6 post 1 28.55180\n27       7 post 1 40.57476\n28       8 post 1 68.97634\n29       9 post 1 19.98486\n30      10 post 1 29.06405\n31       1 post 2 41.10230\n32       2 post 2 69.52441\n33       3 post 2 19.29056\n34       4 post 2 29.49874\n35       5 post 2 38.37091\n36       6 post 2 68.83238\n37       7 post 2 17.81996\n38       8 post 2 28.65901\n39       9 post 2 39.70571\n40      10 post 2 69.53410\n\n\nIt would be helpful if we could see the means of each group, and plot those means.\nFor this, we will use a new function called tappy. This applies a specified function to whatever variables you provide in the arguments.\nTo get the means:\n\nwithin_means <- tapply(within_anova$score,within_anova$groups,mean)\nwithin_means\n\n    post   post 1   post 2      pre \n42.88183 36.61205 42.23381 36.61684 \n\n\nTo get the standard deviations:\n\nwithin_sd <- tapply(within_anova$score,within_anova$groups,sd)\nwithin_sd\n\n    post   post 1   post 2      pre \n20.47817 18.96593 20.24584 18.49391 \n\n\nFor now, it will only be important to use tapply in order to get the means.\nHere is how we will plot the means:\n\nplot(within_means,\n     pch=19,\n     xlab = \"Treatments\",\n     ylab = \"Survey Score\",\n     main = \"Means of Survey Score by Treatment\")\n\n\n\n\nPerforming the ANOVA requires two new terms: Error and the Subject factor.\nThe model that the ANOVA should resemble looks like this:\naov(DV ~ IV+ Error(Subjects))\nFrom here it is important to note that your IV must be a factor.\n\nWith this in mind, you should run the function levels(Grouping Variable) or factor(Grouping Variable.)\nIf the first function returns NULL, use the second function.\n\nPutting all of this together gives us the following:\n\nwithin_anova.mod <- aov(score ~ groups + Error(subject))\n\nsummary(within_anova.mod)\n\n\nError: subject\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  1  113.9   113.9               \n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(>F)\ngroups     3    355   118.4   0.303  0.823\nResiduals 35  13665   390.4               \n\n\nWhen reporting these findings, you will ignore the subject variable and report the F-value for the grouping variable.\nThis particular finding should be reported as follows:\n\nThe results of a one-factor within subjects ANOVA, F(3,35) = .303, p >.05, revealed that there does not appear to be any effect of treatment on survey score."
  },
  {
    "objectID": "Lab-11.html#effect-sizes",
    "href": "Lab-11.html#effect-sizes",
    "title": "Lab 11",
    "section": "Effect Sizes",
    "text": "Effect Sizes\nMost times you will want to report effect sizes for your experiment. Effect sizes help to tell you how much of your effect is due to your manipulation.\nIn this example, there does not appear to be an effect at all, but we will compute an effect size anyways.\nWe will be using the \\(\\omega^2\\) (omega-squared) effect size estimate.\nThe formula is as follows:\n\\[\\omega^2 = \\frac{SS_B-(k-1)(MS_W)}{SS_T-MS_W}\\]\nor\n\\[\\omega^2 = \\frac{SS_{Effect}-(k-1)(MS_{Within})}{SS_{Total}-MS_{Within}}\\]\nThere is no R function that reports \\(\\omega^2\\), so we will do this by hand.\n\\(\\omega^2 = \\frac{355-(4-1)(390.4)}{14020-390.4}\\)\n\\(\\omega^2 = -.059\\)\nThis effect size is negative because our effect was not significant, but it is still important for you to see how to get these numbers!"
  },
  {
    "objectID": "Lab-12.html",
    "href": "Lab-12.html",
    "title": "Lab 12",
    "section": "",
    "text": "In this class we have learned the following, parametric tests, that is, tests that work when we have data on an interval or ratio scale:\n\nZ-test\nT-Test\n\nSingle Sample\nDependent Samples\nIndependent Samples\n\n\nOne Factor ANOVA\n\nBetween\nWithin\n\n\nTwo Factor ANOVA\n\nBetween\n\n\n\nThese tests all share something in common in that our results are estimates of a parameter, and they all are derived from a distribution.\nAdditionally, these tests make assumptions about where the data comes from, i.e. normally distributed, equal variances, etc.\nWhat happens when our data does not meet these criteria? Do we throw our data out? No!\nWe use non-parametric tests. These tests are distribution-free. Think of these tests as another version of what we have already learned.\n\n\nInstead of a Paired t-test, we can use a Wilcoxin Signed Ranks test.\nInstead of a Pearson correlation, we can use a Spearman correlation.\nInstead of an Independent T-Test, we can use a Mann-Whitney U test.\nInstead of a One Way ANOVA, we can use a Kruskal Wallis Test instead.\nInstead of a Two Way ANOVA, we can use a Friedman Test.\n\nOne of the non-parametric tests that does not have a direct equivalence is the chi square test. For this lab you will only need to know about two.\n\nThe Chi Square Goodness of Fit test\nThe Chi-Square Test for Independence.\n\n\nThe Goodness of fit test compares whether observed distribution matches an expected distribution.\nImagine that we go out into a wealthy neighborhood and we count 81 Teslas, 50 Ferrari’s, and 27 Saturn’s.\nAre these car makes equally common in this neighborhood?\nIf these car makes were equally common in wealthy neighborhoods, the proportion of them would be \\(\\frac{1}{3}\\) each.\nHowever, in this wealthy part of town, the breakdown should be:\n\n\\(\\frac{1}{2} = Tesla\\)\n\\(\\frac{1}{3} = Ferrari\\)\n\\(\\frac{1}{6} = Saturn\\)\n\nIs there a significant difference between the observed frequencies and the expected frequencies?\nIn R, we can use the chisq.test(x,p) function where:\n\nx = a numeric vector\np represents probabilities of the same length as x\n\nTo see if the car makes are equally common in the neighborhood we would create our data:\n\n# First, we create a vector of our observed frequencies\ncars <- c(81,50,27)\n\n# Next, we create a vector of the probabilities assuming the null is  true. \np <- c(1/3,1/3,1/3)\n\n# Lastly, we name the function 'res', so we can call it later in more detail. Think of why we label regressions and ANOVA's as 'something.mod'. \nres <- chisq.test(cars,p=p)\n\nres\n\n\n    Chi-squared test for given probabilities\n\ndata:  cars\nX-squared = 27.886, df = 2, p-value = 8.803e-07\n\n\n\nThe proper way to report this would be as follows:\n\\(\\chi^2(2) = 27.886, p <.01\\)\nNow we ask whether or not there is a difference in what we observed and what we expected.\n\n# If the null were true and the proportion of cars were each 1/3, this is what we would have found\nres$expected\n\n[1] 52.66667 52.66667 52.66667\n\n\nAre these observed frequencies significantly different from the expected frequencies?\n\n# Create vector of our observations\ncars <- c(81,50,27)\n# Create vector of our expectations\nobs <- c(1/2,1/3,1/6)\n# Create output of our test\nres <- chisq.test(cars,p=obs)\nres\n\n\n    Chi-squared test for given probabilities\n\ndata:  cars\nX-squared = 0.20253, df = 2, p-value = 0.9037\n\n\n\n\\(\\chi^2(2), .202, p > .05\\)\n\nThere is not a significant different in our observed frequencies and the expected frequencies.\n\n\nSimilarly, we can use a Chi Square test to see if there is a significant difference in two different groups. For this we will need to make a table.\n\n# Create a 3x3 matrix\nmat <- matrix(c(100,100,120,24,25,26,34,13,12),nrow = 3,ncol = 3)\n\n# Name the rows\nrownames(mat) <- c(\"High\",\"Medium\",\"Low\")\n\n# Name the columns\ncolnames(mat) <- c(\"First\",\"Second\",\"Third\")\n\nmat\n\n       First Second Third\nHigh     100     24    34\nMedium   100     25    13\nLow      120     26    12\n\n# Calculate Chi\nchisq.test(mat)\n\n\n    Pearson's Chi-squared test\n\ndata:  mat\nX-squared = 16.025, df = 4, p-value = 0.002986\n\n\n\n\\(\\chi^2(4) = 16.025, p <.01\\)\n\nSpearman Rho is another way of obtaining a correlation. Let us make a data frame, plot the variables and calculate a correlation value:\n\n# Create vector of two professionals\nProfessional1 <- c(6,5,7,10,2.5,2.5,9,1,11,4,11,12)\n\nProfessional2 <- c(5,3,4,8,1,6,10,2,9,7,8,12)\n\n# Combine into dataframe\nspdf <- data.frame(Professional1,Professional2)\n\n# Plot\nplot(spdf$Professional1~spdf$Professional2,pch=20)\n\n\n\n# Calculate correlation\ncor.test(x=spdf$Professional1,y=spdf$Professional2,method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  spdf$Professional1 and spdf$Professional2\nS = 52.274, p-value = 0.001172\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8172245 \n\n\n\n\nrs(12) =.81, p < .05\n\n\nWe can use this test when comparing two independent groups:\n\n\n\n\n\n\nNote: Neighborhood designation is only for demonstrative purposes and should not be an empirical method\n\n\n\n\n\n\n\n# Create two groups with 12 ratings\nGoodNeighborhood <- c(3, 4, 5, 2, 2, 3, 4, 2, 4, 2, 1, 4)\nBadNeighborhood <- c(4, 4, 2, 1, 1, 5, 5, 1, 1, 5, 1, 4)\n\n# Combine into a dataframe\nNeighborhoods <- data.frame(GoodNeighborhood,BadNeighborhood)\n\n# Plot and test\nboxplot(GoodNeighborhood,BadNeighborhood,data=Neighborhoods)\n\n\n\nwilcox.test(GoodNeighborhood,BadNeighborhood,data=Neighborhoods)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  GoodNeighborhood and BadNeighborhood\nW = 77, p-value = 0.7895\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\\(U_{obt} = 77, NS\\)\n\nWe can use the Wilcoxin Signed Ranks Test when comparing two paired dependent groups:\n\n# Create two condition vectors with 12 samples\nCondition1 <- c(75,66,78,66,25,88,89,60,70,49,88,12)\nCondition2 <- c(88,55,78,67,56,85,96,54,97,76,91,13)\n\n# Combine in dataframe\ntwelve_nin <- data.frame(Condition1,Condition2)\ntwelve_nin\n\n   Condition1 Condition2\n1          75         88\n2          66         55\n3          78         78\n4          66         67\n5          25         56\n6          88         85\n7          89         96\n8          60         54\n9          70         97\n10         49         76\n11         88         91\n12         12         13\n\n# Test\nwilcox.test(x=twelve_nin$Condition1,twelve_nin$Condition2,paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  twelve_nin$Condition1 and twelve_nin$Condition2\nV = 15.5, p-value = 0.1301\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\\(V(12) = 15.5, p >.05\\)\n\nWe can use the Kruskal Wallis Test when we want to test several independent variables.\nImagine we have the following data:\n\n# Create vector of 4 food ratings with 7 ratings each\nFood_1 <-c(\"good\",\"good\",\"good\",\"ok\",\"ok\",\"poor\",\"good\")\nFood_2 <-c(\"good\",\"superior\",\"excellent\",\"ok\",\"excellent\",\"excellent\",\"good\")\nFood_3 <-c(\"good\",\"superior\",\"excellent\",\"ok\",\"ok\",\"ok\",\"good\") \nFood_4 <-c(\"superior\",\"excellent\",\"good\",\"excellent\",\"excellent\",\"superior\",\"superior\")\n\n# Combine into dataframe\nFoodRating <- data.frame(Food_1,Food_2,Food_3,Food_4)\nFoodRating\n\n  Food_1    Food_2    Food_3    Food_4\n1   good      good      good  superior\n2   good  superior  superior excellent\n3   good excellent excellent      good\n4     ok        ok        ok excellent\n5     ok excellent        ok excellent\n6   poor excellent        ok  superior\n7   good      good      good  superior\n\n\nThe problem with this data is that we cannot directly do an analysis because the data type is character. We can recode this like such:\n\nFood_1 <- c(3,3,3,2,2,1,3)\nFood_2 <- c(3,5,4,2,4,4,3)\nFood_3 <- c(3,5,4,2,2,2,3)\nFood_4 <- c(5,4,3,4,4,5,5)\nNFoodRating <- data.frame(Food_1,Food_2,Food_3,Food_4)\nNFoodRating\n\n  Food_1 Food_2 Food_3 Food_4\n1      3      3      3      5\n2      3      5      5      4\n3      3      4      4      3\n4      2      2      2      4\n5      2      4      2      4\n6      1      4      2      5\n7      3      3      3      5\n\nkruskal.test(NFoodRating)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  NFoodRating\nKruskal-Wallis chi-squared = 10.677, df = 3, p-value = 0.01361\n\n\n\n\\(H(4) = 10.677, p <.05\\)\n\nWe can use a Friedman Test when we want to compare the effect of two or more independent variables on a dependent variable like such:\nRemember: This test is designed to replicate the two-way ANOVA, just with different data types, because of this your data needs to look similar.\n\n# Create Vector of groups\na <- rep(c(\"Thing 0\",\"Thing 1\",\"Thing 10\",\"Thing 100\"),each=12)\n\n# Create score vector\ny <- round(runif(48,min=c(11.1,11.2,11.1,11.6),\n                 max = c(11.7,11.7,22.6,18)),\n           digits=1)\n\n# Create participant vector\nb <- as.factor(rep(1:12,4))\n\n# Calculate test\nfried_tst <- data.frame(a,b,y)\nfried_tst\n\n           a  b    y\n1    Thing 0  1 11.3\n2    Thing 0  2 11.2\n3    Thing 0  3 21.6\n4    Thing 0  4 17.9\n5    Thing 0  5 11.2\n6    Thing 0  6 11.3\n7    Thing 0  7 20.4\n8    Thing 0  8 14.3\n9    Thing 0  9 11.4\n10   Thing 0 10 11.3\n11   Thing 0 11 11.2\n12   Thing 0 12 15.1\n13   Thing 1  1 11.4\n14   Thing 1  2 11.3\n15   Thing 1  3 11.8\n16   Thing 1  4 17.4\n17   Thing 1  5 11.4\n18   Thing 1  6 11.3\n19   Thing 1  7 22.5\n20   Thing 1  8 17.4\n21   Thing 1  9 11.3\n22   Thing 1 10 11.4\n23   Thing 1 11 19.9\n24   Thing 1 12 13.4\n25  Thing 10  1 11.2\n26  Thing 10  2 11.4\n27  Thing 10  3 11.4\n28  Thing 10  4 11.6\n29  Thing 10  5 11.4\n30  Thing 10  6 11.5\n31  Thing 10  7 15.3\n32  Thing 10  8 12.9\n33  Thing 10  9 11.2\n34  Thing 10 10 11.7\n35  Thing 10 11 16.9\n36  Thing 10 12 14.3\n37 Thing 100  1 11.4\n38 Thing 100  2 11.5\n39 Thing 100  3 14.3\n40 Thing 100  4 13.3\n41 Thing 100  5 11.5\n42 Thing 100  6 11.3\n43 Thing 100  7 12.6\n44 Thing 100  8 14.6\n45 Thing 100  9 11.6\n46 Thing 100 10 11.7\n47 Thing 100 11 19.2\n48 Thing 100 12 16.4\n\nfriedman.test(y ~ a|b)\n\n\n    Friedman rank sum test\n\ndata:  y and a and b\nFriedman chi-squared = 5.2035, df = 3, p-value = 0.1575\n\n\n\n\\(fr_{obt} (3), = 5.362, ns\\)"
  },
  {
    "objectID": "Lab-2.html",
    "href": "Lab-2.html",
    "title": "Lab 2",
    "section": "",
    "text": "Welcome back!\nHow did everyone find the exercises? Were they too difficult? Were they too easy? Were there any parts that you didn’t understand? Each week when an exercise is turned in I would like to take time in the beginning of lab to make sure that everyone is on the same page. Statistics, especially when using an unfamiliar program like R, can be quite difficult. If you fall behind it is essential that you let me know so I know what things to stress.\nFirst, let us do some review.\n\nWhat would I need to type into R to have it display a histogram of the dataset: 1,2,3,4,5,6,7,8,9,10?\nHow could I rename the x-axis on this plot so that it carried more information than it currently does?\n\n\nx = c(1,2,3,4,5,6,7,8,9,10,11)\ny = c(1,4,9,16,25,36,49,64,81,100,121)\nplot(y~x, main = \"Years Spent in School vs. Salary\", ylab = \"Salary\")\n\n\n\n\n\nUh-oh! I had just finished typing in a really long function and I accidentally forgot a comma. Is there any way for me to get everything I typed back without needing to retype it?\nGiven the following dataset, create a plot that shows the relationship between x and y, y and x, and z and y:\n\nx = {2,4,6,10,16,26,42} y = {1,3,5,7,9,11,13} z = {5,10,15,20,25,30,35}\n\nx = c(2,4,6,10,16,26,42)\ny = c(1,3,5,7,9,11,13)\nz = c(5,10,15,20,25,30,35)\n\n\ndf<-data.frame(x,y,z)\nplot(df$x~df$y)\n\n\n\nplot(df$y~df$x)\n\n\n\nplot(df$z~df$y)\n\n\n\n\n\nBelow are two plots. Of the two, which one do you think shows a more apparent relationship or pattern?\n\n\nx=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)\ny=c(2,2,7,9,11,13,15,17,11,20,22,24,27,28,31,32,35,37,38,40)\nplot(x~y, main = \"Plot 1\", ylab = \" \", xlab = \" \")\n\n\n\na=rnorm(20,20,5)\nb=rnorm(20,100,10)\nplot(a~b, main = \"Plot 2\", ylab = \" \", xlab = \" \")\n\n\n\n\nAny questions?\nMean, Standard Deviation, Variance, Z-Score, Summation, and Stem and Leaf Plots\nYou will see the formula for the arithmetic mean in several different ways. Here are just a few.\n\\(\\mu = \\frac{\\sum x}{n}\\) or \\(M = \\frac{x_1 + x_2 + x_3...}{N}\\)\nIn other words, all numbers in a set should be added together and divided by the number of items in the set.\nStandard Deviation\n\\(\\sigma = \\sqrt\\frac{\\sum (x - \\mu)^2 }{n}\\)\nVariance \\(\\sigma^2 = \\frac{\\sum (x - \\mu)^2}{n}\\)\n\n\n\n\n\n\nNote: Variance is standard deviation squared!\n\n\n\n\n\n\nThis is the formula used to find a z score.\nZ is equal to the raw score \\(x\\) minus the mean \\(\\mu\\), divided by the standard deviation \\(\\sigma\\).\n\\(z =\\frac{x-\\mu}{\\sigma}\\)\nFirst, we can calculate z-scores by hand:\n\nx = c(12,16,20,41)\nmx = mean(x)\nsdx = sd(x)\n\nz = ((x-mx)/sdx)\nz \n\n[1] -0.7933668 -0.4837602 -0.1741537  1.4512807\n\n\nNext, we can use the scale function and compare our results:\n\nx = c(12,16,20,41)\nscale(x)\n\n           [,1]\n[1,] -0.7933668\n[2,] -0.4837602\n[3,] -0.1741537\n[4,]  1.4512807\nattr(,\"scaled:center\")\n[1] 22.25\nattr(,\"scaled:scale\")\n[1] 12.91962\n\n\nHere are a few practice problems:\n\nFind the mean, standard deviation, and variance for the following dataset:\n\ndatset1 = {10,63,51,24,87,42}\n\nCodedatset1 = c(10,63,51,24,87,42)\nmds = mean(datset1)\nsdds=sd(datset1)\nvards=(sdds^2)\nvards\n\n[1] 758.1667\n\nCodevar(datset1)\n\n[1] 758.1667\n\n\n\nFind the z scores for the same dataset.\n\n\nz = ((datset1-mds)/sdds)\nz\n\n[1] -1.3134881  0.6113470  0.1755353 -0.8050411  1.4829704 -0.1513235\n\n\n\nPerform the following summations on this dataset:\n\nx = (5,8,1,16,4,11)\n\n\\(\\sum x^2\\)\n\\(\\sum(x)^2\\)\n\n\nx = c(5,8,1,16,4,11)\na = sum(x)^2\nb = sum(x^2)\n\na\n\n[1] 2025\n\nb\n\n[1] 483\n\n\n\nSuppose that a psychologist wants to investigate the scores of his students in his statistics class. He may want to create a stem and leaf diagram in order to visualize the scores.\n\nHere is how he might do this for the following data:\n{85,90,88,95,90,91,85,94,83,86,90,90,88,94,90}\n\n# This allows us to produce the same random numbers each time\nset.seed(100)\nx = round(rnorm(15,89,7), digits = 0)\nx\n\n [1] 85 90 88 95 90 91 85 94 83 86 90 90 88 94 90\n\nstem(x)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  8 | 3\n  8 | 55688\n  9 | 00000144\n  9 | 5\n\n\nFrom this output we can infer a few things:\n\nSeveral students (9) scored somewhere in the 90’.\nNo students failed\nThere are two areas for scores in the 80’s and scores in the 90’s, this is because the first “leaf” represents values from 0-4 and the second “leaf” accounts for the 5-9.\n\nSimilar to histograms, ( hist(x, scale = )) we can manipulate how the stem and leaf plot is displayed using the following phrase: stem(x, scale =)\nWatch what happens when we use the different scale values.\n\nset.seed(10)\nx = round(rnorm(15,20,2),digits = 0)\nx\n\n [1] 20 20 17 19 21 21 18 19 17 19 22 22 20 22 21\n\nstem(x,scale = 1)\n\n\n  The decimal point is at the |\n\n  16 | 00\n  18 | 0000\n  20 | 000000\n  22 | 000\n\nstem(x,scale = 2)\n\n\n  The decimal point is at the |\n\n  17 | 00\n  18 | 0\n  19 | 000\n  20 | 000\n  21 | 000\n  22 | 000\n\nstem(x, scale = .5)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  1 | 778999\n  2 | 000111222\n\n\nThe first stem and leaf plot is the default you will get if you just type in stem(x). The second stem and leaf plot separates each leaf making the list twice as long. The third stem and leaf plot uses half the spacing as the first, so the scale is about half as long."
  },
  {
    "objectID": "Lab-3.html#linear-regression",
    "href": "Lab-3.html#linear-regression",
    "title": "Lab 3",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression refers to an equation of the “best fitting line” that shows where a line could be placed that would best explain the relationship, if any, of your data.\nThe equation is as follows:\n\\(\\hat{Y_x} = a_x + b_yx\\)\nWhere \\(\\hat{Y}\\) equals the predicted value of Y given X.\nIn order to find b:\n\\(b = \\frac{n\\sum XY -\\sum X\\sum Y }{n\\sum X^2-\\sum (X)^2}\\) or \\(b = \\frac{\\sum XY }{\\sum X^2}\\)\nIn order to find a:\n\\(a = \\mathrel{\\bar{Y}}-b\\mathrel{\\bar{X}}\\)\nLet us work with a sample set of data:\nx = 20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100\ny = 10.93,11.78,10.97,11.27,11.37,10.31,12.84,12.15,10.86,13.25,12.43,11.70,12.90 12.8,12.82,12.69,12.55\n\nx = c(20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100)\ny= c(10.93,11.78,10.97,11.27,11.37,10.31,12.84,12.15,10.86,13.25,12.43,11.70,12.90,\n 12.8,12.82,12.69,12.55)\nx\n\n [1]  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95 100\n\ny\n\n [1] 10.93 11.78 10.97 11.27 11.37 10.31 12.84 12.15 10.86 13.25 12.43 11.70\n[13] 12.90 12.80 12.82 12.69 12.55\n\n\n\nSolve for the numerator of a\n\n\\(n\\sum XY -\\sum X\\sum Y\\)\n\n\nlength(x)\n\n[1] 17\n\nlength(y)\n\n[1] 17\n\n\n\ntopb=(sum(x*y))-(sum(x)*(sum(y)/17))\ntopb\n\n[1] 243.25\n\n\n\nSolve for the denominator of b\n\n\\(b = \\frac{243.25}{n\\sum X^2-\\sum (X)^2}\\)\n\n\nbotb = (sum(x^2))-((sum(x)^2)/17)\nbotb\n\n[1] 10200\n\n\n\n\\(b = \\frac{243.25}{10,200}\\)\n\n\nb=topb/botb\nb\n\n[1] 0.02384804\n\n\n\nSolve for a\n\n\\(a = \\mathrel{\\bar{Y}}-b\\mathrel{\\bar{X}}\\)\n\n\na=mean(y)-b*(mean(x))\na\n\n[1] 10.54676\n\n\n\\(\\hat{Y} = 10.546 + .0238x\\)\n\\(\\frac{\\sum(X-\\mathrel{\\bar{X}}^2)\\sum(Y-\\mathrel{\\bar{Y}}^2)}{\\sum(X-\\mathrel{\\bar{X}})^2}\\)\nor\n\\(\\frac{SSxy}{SSx}\\)\n\nssxy=sum((x-mean(x))*(y-mean(y)))\nssxy\n\n[1] 243.25\n\n\n\nssx=sum((x - mean(x))^2)\nssx\n\n[1] 10200\n\n\n\\(\\frac{243.25}{10,200}\\)\n\nb = (ssxy/ssx)\nb = round(b,digits=4)\nb\n\n[1] 0.0238\n\n\n\na=(mean(y))-(b*(mean(x)))\na\n\n[1] 10.54965\n\n\nOur formula is: \\(\\hat{Y} = 10.546 + .0238x\\)\nAs with correlation, there is an ‘easier’ way to find the equation for the best fitting line.\nNow, let us use the lm() function to create the prediction equation. What you will notice is the use of <-, which is another way of assignment. We will also name the equation prediction xy.mod. which stands for model.\n\nxy.mod<-lm(y~x)\nxy.mod = summary(xy.mod)\nxy.mod\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.30993 -0.29221 -0.09373  0.29159  1.15311 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.546765   0.438253  24.065 2.13e-13 ***\nx            0.023848   0.006762   3.527  0.00305 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.683 on 15 degrees of freedom\nMultiple R-squared:  0.4533,    Adjusted R-squared:  0.4168 \nF-statistic: 12.44 on 1 and 15 DF,  p-value: 0.003052\n\n\nFrom the above output you should note a few things: the number across from the (Intercept) will be your value for a. The number across from x will be your value for b. This summary() will also give you other statistics such as the R-squared.\nWe have the equation of the line, so let us draw it To do this we will use the abline() function. The parts of the function asks to specify the a= and b= so the full function should read abline(a=intercept,b=slope). We will add col= red in order to color the line red. There are plenty of graphical options built into R, we will not go too in depth at the moment.\nBefore inserting the function, we will need to plot our data first. When dealing with so-called ‘real’ data, your dependent variable should be the y and the independent should be the x.\n\nplot(y~x,\n     sub=\"Line of Best Fit Y'=10.546+.0238x\")\n\nabline(a=a,b=b,col=\"red\")"
  },
  {
    "objectID": "Lab-4.html#graphical-additions",
    "href": "Lab-4.html#graphical-additions",
    "title": "Lab 4",
    "section": "Graphical Additions",
    "text": "Graphical Additions\nWe have already seen the basics of using graphics to visualize our data with functions such as plot() and hist().Additionally, we have seen that we can do certain things to clean-up our visualizations such as providing titles and captions through the use of main=\"Title\", sub=\"Caption\" and respectively, adding axis labels with xlab=\"X\", ylab=\"Y\". `\nWe have also seen how to draw a line through the points that would best explain the relationship present if any, in the data through the use of abline(a=,b=).\n\nset.seed(105)\nvar1=rnorm(10,30)\nvar2=rnorm(10,23)\nplot(var1,var2,main=\"Some Nice Title\",xlab = \"Variable 1\",ylab=\"Variable 2\", sub=\"r = .64 \" )\n\n\n\nvar.mod<-lm(var2~var1)\nsummary(var.mod)\n\n\nCall:\nlm(formula = var2 ~ var1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22571 -0.00482  0.11789  0.26722  0.44729 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   6.0602     7.0975   0.854   0.4180  \nvar1          0.5510     0.2368   2.327   0.0484 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5139 on 8 degrees of freedom\nMultiple R-squared:  0.4036,    Adjusted R-squared:  0.3291 \nF-statistic: 5.414 on 1 and 8 DF,  p-value: 0.0484\n\n\nWe can tell that this line is equal to: \\(Y= 6.060 + .551\\)\nIn our discussion on linear regression we discussed how to add the line of best fit through the use of the abline() function. We can color the line through the use of col=\" \", and we can change the linetype by changing the value of lty.\n\nplot(var1,var2)\nabline(a=6.060,b=.551,col=\"red\", lty = 2)\n\n\n\n\nR has quite the selection of colors available to you. The colors() function, you will see an output of 657 possible colors. Here are just a few:\n\ncl=colors()\nsample(cl,10)\n\n [1] \"navajowhite\"     \"cyan3\"           \"gray28\"          \"mediumslateblue\"\n [5] \"gray65\"          \"deeppink3\"       \"goldenrod\"       \"sienna\"         \n [9] \"darkviolet\"      \"plum4\"          \n\n\nIf you can think of a color, there is a name for it in R (most likely!)\nPoint characters\nPoint characters, or as they are named in R pch allow you to change how a point looks. Here is an example:\n\nset.seed(71)\nx=rnorm(100,50)\ny=rnorm(100,25)\nplot(x,y,pch=2)\n\n\n\n\nHere is an image of some of the possible choices you will can use. \nIf I wanted to plot a graph with green, filled in squares, I would do this:\n\nset.seed(60)\nx=rnorm(15,100);y=rnorm(15,50)\nplot(x,y,col=\"green\",pch=15)\n\n\n\n\nOkay, so we can add colors and shapes to our graphs. So what? What if we wanted to display certain information in a certain way, or what if we wanted to separate our data in some distinguishing way?\nIf you are presented with a dataset with more than one independent variable, it can be difficult to discern any relationship when you have to graph it twice. Additionally we will calculate the linear regression summaries of each independent variables:\n\nset.seed(10)\n\nAttention_Experiment<-data.frame(Age=round(rnorm(50,17,2),digits = 0),Minutes_Phone=round(rnorm(50,120,5),digits=2),Test_Score=round(rnorm(50,80,10),digits=2))\n\nplot(Attention_Experiment$Test_Score~Attention_Experiment$Age,xlab=\"Age\",ylab=\"Test Score\")\n\n\n\nplot(Attention_Experiment$Minutes_Phone~Attention_Experiment$Age,xlab=\"Age\",ylab=\"Hours TV Watched\")\n\n\n\nTest_Age.mod<-lm(Attention_Experiment$Test_Score~Attention_Experiment$Age)\n\nPhone_Use.mod<-lm(Attention_Experiment$Minutes_Phone~Attention_Experiment$Age)\n\nsummary(Test_Age.mod)\n\n\nCall:\nlm(formula = Attention_Experiment$Test_Score ~ Attention_Experiment$Age)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.509  -7.738   2.219   7.314  14.414 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               78.4391    12.9521   6.056 2.06e-07 ***\nAttention_Experiment$Age   0.1105     0.7900   0.140    0.889    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.82 on 48 degrees of freedom\nMultiple R-squared:  0.0004075, Adjusted R-squared:  -0.02042 \nF-statistic: 0.01957 on 1 and 48 DF,  p-value: 0.8893\n\nsummary(Phone_Use.mod)\n\n\nCall:\nlm(formula = Attention_Experiment$Minutes_Phone ~ Attention_Experiment$Age)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3595 -3.3896  0.4043  3.2590 10.7080 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              120.59529    6.50609  18.536   <2e-16 ***\nAttention_Experiment$Age  -0.01564    0.39685  -0.039    0.969    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.933 on 48 degrees of freedom\nMultiple R-squared:  3.235e-05, Adjusted R-squared:  -0.0208 \nF-statistic: 0.001553 on 1 and 48 DF,  p-value: 0.9687\n\n\nWe could take the correlation using cor() and see what the value of R-Squared was and keep looking back and forth, but there is a (somewhat) easier way!\nFor this we wil be introducing three new functions: points(), xlim=',ylim=,andhead()`.\nFirst: head() will show us a preview of our data, from here we can see where the values tend to lie, and get an idea of where our axes should be.\n\nhead(Attention_Experiment,10)\n\n   Age Minutes_Phone Test_Score\n1   17        118.00      72.38\n2   17        118.33      84.19\n3   14        126.84      69.60\n4   16        130.69      87.12\n5   18        122.53      73.67\n6   18        123.93      85.63\n7   15        115.49      86.61\n8   16        122.66      63.42\n9   14        116.77      90.28\n10  16        121.45      91.28\n\n\nAge has a minimum of 14 and a maximum of 18.Hours Watched has a minumum of 5.05 and maximum of 7.10. Test Score has a minumum of 58.81 and a maximum of 78.98.\nWe can use this information to create our new axis limits using xlim and ylim. Additionally. the points() function will be used in order to plot additional variables onto the existing plot. Finaly, we will use abline() to draw the line of best fit for each dataset.\n\n# Plot the first relationship\nplot(Attention_Experiment$Test_Score~Attention_Experiment$Age,\n     xlab=\"Age\",\n     ylab=\"Test Score and Minutes on Phone\",\n     xlim=c(10,20),\n     ylim=c(20,150),\n     col=\"#023047\",\n     pch=15)\n\n# Plot the second relationship\npoints(Attention_Experiment$Minutes_Phone~Attention_Experiment$Age,\n       xlab=\"Age\",\n       ylab=\"Hours TV Watched\",\n       xlim=c(10,20),ylim=c(20,150),\n       col=\"#2F4602\",\n       pch=17)\n\n# Create the legend\nlegend(\"bottomleft\",\n       title=\"Legend\",c(\"Phone Usage\",\"Test Score\"),\n       fill=c(\"#023047\",\"#2F4602\"),cex = .9)\n\n\n# Add in line of best fit\nabline(Test_Age.mod,col=\"#2F4602\")\n\nabline(Phone_Use.mod,col=\"#023047\")\n\n\n\n\nThat was a lot to squeeze into two lines so let’s just go over it one last time.\n\n\nplot() Draw a graph of your points, it accepts the following inputs:\n\n\nx,y:The independent and dependent variable you are interested in\n\nmain,xlab, ylab, sub: Title, x-axis lable, y-axis label, and caption.\n\nxlim,ylim: These set parameters for the graph that will be drawn.\n\ncol : Designates a desired color for the lines, and points that will be drawn.\n\npch: Point character, assigns shape for points drawn.\n\ncex: Size of the legend\n\n\n\npoints(): Plot additional points on an existing graph with pre-defined graphical parameters."
  },
  {
    "objectID": "Lab-5.html#proportioning-data",
    "href": "Lab-5.html#proportioning-data",
    "title": "Lab 5",
    "section": "Proportioning Data",
    "text": "Proportioning Data\nThe below code will allow you to take a set of data and see what proportion is accounted for given a certain criteria.\nFor example: If I have a data set of 1’s and 0’s, how can I know how many I have of each. Additionally, how can I tell the proportion of each?\nTo do this we need to use the which()command as well as how much of the data is made up of 1’s and 0’s.\n\n# Create the dataset\nx=c(0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,0,1,0,1,0,1,1,1,0,1,0,1)\n\nx\n\n [1] 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1\n\none = length(which(x==1))\n\none\n\n[1] 14\n\nzero = length(which(x==0))\n\nzero\n\n[1] 13\n\nlength(x)\n\n[1] 27\n\none + zero\n\n[1] 27\n\none/length(x)\n\n[1] 0.5185185\n\nzero/length(x)\n\n[1] 0.4814815\n\n\nWe can see here that the amount of 1’s is quite close to the amount of 0’s. Now let us try with a more involved example.\n\n# Make a dataset of differnt colored marbles\nd=c(\"red\",\"red\", \"blue\",\"orange\", \"orange\",\"orange\") \n\n# Repeat this arrangement 20 times\nx= rep(d,20) \n\n# What is the length of x that contains \"red\"\nrl=length(x[x==\"red\"])\nrl\n\n[1] 40\n\n# What is the length of x that contains \"blue\"\nrb=length(x[x==\"blue\"])\nrb\n\n[1] 20\n\n# What is the length of x that contains \"orange\"\nro=length(x[x==\"orange\"])\nro\n\n[1] 60\n\n# How many colors do we have?\nrl+rb+ro \n\n[1] 120\n\n# How many colors do we have(easy way)?\nxl=length(x) \nxl\n\n[1] 120\n\n# What is the prportion of red compared to the entire dataset\nrp=rl/xl \nrp\n\n[1] 0.3333333\n\n# What is the prportion of blue compared to the entire dataset\nbp=rb/xl\nbp\n\n[1] 0.1666667\n\n# What is the prportion of orange compared to the entire dataset\nop=ro/xl\nop\n\n[1] 0.5\n\n# Draw a barplot with these different proportions\nbarplot(c(rp,bp,op),\n        col=c(\"red\",\"blue\",\"orange\"),\n        names.arg=c(\"Red\",\"Blue\", \"Orange\"))\n\n\n\n\nrnorm()\nIn most, if not all of my slides, you have seen me use the function rnorm(). In class, I’ve used this as a way to generate large sets of data with one line of text. There are a few arguments that are necessary in order to take this function and use it for the purposes of sampling.\nrnorm() stands for “Random Normal”. Essentially, what rnorm() does it create a random set of numbers (specified by you) and generates data. pulled from a normal distribution.\nHere are the arguments rnorm() accepts:\n\nn: Specifies the number of data points you want to create.\nmean: Specifies the mean you want your sample to have.\nsd: Specifies the standard deviation you want your sample to have.\n\nAlthough not essential, a nice function to use is round(digits = 0)\n\nThis function makes your data look a little bit neater, you will see that when we create a sample from rnorm() the numbers are a little messy.\n\nSo, let’s make a sample of 20 numbers with a mean of 12 and a standard deviation of 3.\n\nx = rnorm(20,12,3)\nx\n\n [1] 10.493423 12.394593 11.763249 14.660354 12.350914 12.955890 10.254628\n [8] 14.143598  9.524222 10.920414 12.269658 12.288823 11.395098 14.219521\n[15] 12.370139 11.912050 10.833437 13.532569  9.258557 18.930890\n\n\nSee how the numbers came out a little unwieldy? Let’s fix that:\n\nx= round(rnorm(20,12,3),digits=2)\nx\n\n [1] 10.69 14.29 12.79 14.32  9.56 10.68  9.84 12.69  8.53 12.74 11.73 17.27\n[13] 11.59 11.67  9.93 11.33 12.55 13.25 15.20 14.91\n\n\nNotice how the digits= command is preceded by a comma. This is because it is a part of the round() function but not a part of the rnorm() function.\nWe can specify the mean and the standard deviation of the dataset, but does introducing the variability of the standard deviation make it harder for the mean to be exactly what we asked for? Let’s see!\n\nx=rnorm(100,12,3)\nx = round(x, digits = 2)\nmean(x)\n\n[1] 11.8655\n\n\nSampling\nWe can have our dataset of 100 points, but if we take a sample of that data, how representative will it be of the true mean?\nLet’s take a look:\n\n# For reproduible examples\nset.seed(11)\n# Create a sample of 10,000 with a mean of 10 and standard deviation of 10\nx=round(rnorm(10000,10,15),digits = 2)\n\n# Sample 1000\ns1=sample(x,1000)\n\n# Sample 100\ns2=sample(x,100)\n\n# Sample 10\ns3=sample(x,10)\nmean(s1)\n\n[1] 10.41259\n\nmean(s2)\n\n[1] 11.3497\n\nmean(s3)\n\n[1] 12.371\n\nmean(x)\n\n[1] 10.17055\n\n\nDo you see how it becomes difficult to tell a certain property of a random sample when you only have small sample to use?"
  },
  {
    "objectID": "Lab-6.html#packages",
    "href": "Lab-6.html#packages",
    "title": "Lab 6",
    "section": "Packages",
    "text": "Packages\nThe wonderful thing about R, is that once you know to use it, you can use libraries that process unlimited numbers of statistics. In order to do this, one needs to learn a few commands in order to access these online libraries and use them. With Exercise 06, we introduce the downloading of the BSDA package. Within this package comes the functions z.test() and SIGN.test().\nFirst, install the package: install.packages(BSDA)\nAfter installing it on your computer, you can use at anytime you want by telling R to use the package. You do this by:library(BSDA)\nZ-Test\nIn order to compute a Z-Test, you will need a data set. This is a single column of numbers, or in R-Speak, a vector.\nThis will be compared against the known population mean \\(\\mu\\) (mu) and standard deviation \\(\\sigma\\) (sigma.x).\nThe z.test() accepts the following arguments:\n\nx - a numeric vector\nmu = - A single number representing the mean.\nsigma.x - A single number representing the population standard deviation for x.\nalternative = \"two.sided\",\"less\",\"greater\" - Specification of the alternative hypothesis.\n\nSo, in review, your use of the z.test() function should look like this:\nz.test(x=, mu=, sigma.x=, alternative = \"two.sided\")\nZ-Test Example\nIn order to perform a Z-Test, you need to have a specified \\(/sigma\\) and \\(/mu\\).\nIn order to find the Standard Error of the mean we need to do the following computation.\n\\[\\sigma_\\bar{x} = \\frac{s}{\\sqrt{n}}\\] We also need to find the mean of the sample:\n\\[\\bar{x{}} = \\frac{\\Sigma X}{n}\\]\nThen we can plug that into the Z formula.\n\\[z = \\frac{\\bar{x}-\\mu}{\\sigma_x}\\]\nLet’s use this in an example.\nDave knows that from his experience working with undergraduate participants on tasks related to attention that the population mean is 71 and the standard deviation is 2.4. He measures the attention scores of 13 participants and finds the following:\n71,74,73,71,78,73,72,71,73,72,74,72,75\n\nFirst, we can find the SEM.\n\n\\[\\sigma_x = \\frac{2.4}{\\sqrt{13}} = .66\\]\n\nThen we will need to find the mean:\n\\(Xbar = \\frac{71+74+73+71+78+73+72+71+73+72+74+72+75}{13} = 73\\)\nWe can then plug that into the Z formula:\n\\(Z_{obt} = \\frac{73-71}{.66} = 3.0\\)\nThen we can look in the Z-Table(link) for the \\(Z_{crit}\\) of .05 is 1.64.\n\nBecause \\(Z_{obt} > Z_{crit}\\), we can reject the null hypothesis that there is no difference.\nWe can write this as:\n\\(Z_{obt}(13) = +3.03 < .05\\)\nNow we can see how R tackles the same problem.\n\n# Load in the BSDA Package\nlibrary(BSDA)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'BSDA'\n\n\nThe following object is masked from 'package:datasets':\n\n    Orange\n\n# Create our sample vector\nx=c(71,74,73,71,78,73,72,71,73,72,74,72,75)\n\n# Plug in the numbers\nz.test(x,sigma.x=2.4,alternative=\"two.sided\", mu=71)\n\n\n    One-sample z-Test\n\ndata:  x\nz = 3.0046, p-value = 0.002659\nalternative hypothesis: true mean is not equal to 71\n95 percent confidence interval:\n 71.69537 74.30463\nsample estimates:\nmean of x \n       73 \n\n\nThe output that is given by the z.test() function gives us much more than we need for right now. Note that the Z value is slightly different, this is mainly due to how th computer parses the data. You will also see that this output gives an exact value for p. For right now, stick to indicating if your \\(Z_{obt}\\) is > or < \\(\\alpha{_.05}\\).\nSign Test\nFor the sign test, we will be determining whether or not a difference is due to chance. The sign test uses the number of pluses + and minuses - to determine the strength of an effect. We will be using the BSDA package again, specifically the SIGN.test() function.\nThe SIGN.test() function accepts the following arguments:\n\nx - a vector of difference scores\nmd = - This is your cut-off point. Unless specified, a common cut-off point is 0. If there was no effect, there would be a difference of 0 between 1 treatment and and another.\nalternative = - Much like the z.test, this can be either two.sided, lesser or greater. You can think of the latter two as being equal to one tailed negative and one tailed positive.\n\nYour full SIGN.test()function should look something like this:\nSIGN.test(x = , md = , alternative = \" \")\nSign Test Example\nA researcher is interested to see whether his students respond perform better on tests when they listen to metal music playing while they study compared to when they study with classical music. He takes 10 students and measures their scores on an exam after they listened to metal music while studying. He then later collects their scores on their tests while they listened to classical music, he then measures the difference. Using an \\(\\alpha\\) level of .05, what can he conclude?\nTo visualize this, let us create this data.\n\n# To reproduce\nset.seed(3)\n\n# Create dataframe\nmusic_study <-\n  data.frame(\n    Metal= round(rnorm(10,85,5),digits = 0),\n    Classical=round(rnorm(10,78,8),digits=0),\n    Difference=c(\"+\",\"+\",\"+\",\"-\",\"+\",\"+\",\"+\",\"+\",\"-\",\"+\"))\n\nmusic_study\n\n   Metal Classical Difference\n1     80        72          +\n2     84        69          +\n3     86        72          +\n4     79        80          -\n5     86        79          +\n6     85        76          +\n7     85        70          +\n8     91        73          +\n9     79        88          -\n10    91        80          +\n\n\nWe cannot do any analyses on character data right now, so let’s make sure to convert those values to numbers just so we can see.\n\n# To reproduce\nset.seed(3)\n\nmusic_study <- \n  data.frame(\n    Metal=round(rnorm(10,85,5),digits = 0),\n    Classical=round(rnorm(10,78,8),digits=0)\n  )\n\n# Once the dataframe is created, you can add additional columns\nmusic_study$Diff = music_study$Metal-music_study$Classical\nmusic_study\n\n   Metal Classical Diff\n1     80        72    8\n2     84        69   15\n3     86        72   14\n4     79        80   -1\n5     86        79    7\n6     85        76    9\n7     85        70   15\n8     91        73   18\n9     79        88   -9\n10    91        80   11\n\n\nSo we have two minuses and 8 pluses. What is the probability associated with this outcome with a sample of 10? In other words, think of this as \\(p(8)\\) which necessarily includes \\(p(9)\\) and \\(p(10)\\).\n\nlibrary(BSDA)\nSIGN.test(music_study$Diff,alternative = \"greater\", md=0)\n\n\n    One-sample Sign-Test\n\ndata:  music_study$Diff\ns = 8, p-value = 0.05469\nalternative hypothesis: true median is greater than 0\n95 percent confidence interval:\n 6.146667      Inf\nsample estimates:\nmedian of x \n         10 \n\nAchieved and Interpolated Confidence Intervals: \n\n                  Conf.Level  L.E.pt U.E.pt\nLower Achieved CI     0.9453  7.0000    Inf\nInterpolated CI       0.9500  6.1467    Inf\nUpper Achieved CI     0.9893 -1.0000    Inf\n\n\nThe researcher can conclude that there appears to be no significant difference in test results from students who listen to metal music while studying compared to students who listened to classical music while studying.\nWe can see from the output that we are just shy of being significant. It will be good practice to be rather dichotomous, much like the sign test in saying you are either below your \\(\\alpha\\) level, or above it.\nNow, let’s go backwards a bit: When we calculated a z-test, we did it by “hand” and then by computer. Now let us perform the same sign test by “hand”.\nAll we need to know is that we ended up with 8 pluses (+) and two minuses (-).\nWe can look at the binomial table, in the column for .50 where n = 10.\nWe will need to add the probabilities of \\(p(8)\\),\\(p(9)\\), and \\(p(10)\\) to get our answer:\n\\[p(8)+p(9)+p(10) = .0439 + .0098 + .0010 + = .0547 \\]\nMatrices and Dataframes and Indexing\nWe have learned about vectors and dataframees. We will now learn about matrices.\nVector\n\nx = c(1,2,3,4,5)\nx\n\n[1] 1 2 3 4 5\n\n\nDataframe\n\nx=1:10\ny=x\nxy<-data.frame(x,y)\nxy\n\n    x  y\n1   1  1\n2   2  2\n3   3  3\n4   4  4\n5   5  5\n6   6  6\n7   7  7\n8   8  8\n9   9  9\n10 10 10\n\n\nMatrices\nWhen creating a matrix, you can define your number of columns and number of rows through the use of\nnrow= and ncol=.\n\nmatrix(c(x=1:5,y=c(120,130,131,144,100)),nrow = 5,ncol = 2)\n\n     [,1] [,2]\n[1,]    1  120\n[2,]    2  130\n[3,]    3  131\n[4,]    4  144\n[5,]    5  100\n\n\nOnce you have a matrix you can refer to a specific data point using [,]. The way this works is that anything inside of the left bracket ([) will refer to the row. Anything inside of the right bracket (]) will refer to the column.\nLet’s try it out.\n\nxmat=matrix(c(x=1:5,y=c(120,130,131,144,100)),nrow = 5,ncol = 2)\nxmat[1,1]\n\n[1] 1\n\nxmat[2,2]\n\n[1] 130\n\nxmat[2,1]\n\n[1] 2\n\nxmat[4,2]\n\n[1] 144\n\n\nAPA Style Statistical Reporting\nSo far in this class, we have looked at datasets, and performed analyses. The next step is to report these findings.\nAll of psychology uses APA style writing. Within this writing style there is a special style to report results.\nFrom this point forward, on your homework if you do any analyses and are asked to report your findings, you must use proper statistical formatting in order to get all of the points.\nThe Z-Test deals with significance, as does the Sign Test.\nIn most cases, you will be reporting as follows:\nYou will be reporting probabilities.\np = ns\nor\np = .004\n\nZ-Test: \\(Z_{obt}(n) = (+/-)Score > < = \\alpha\\)\n\nSign Test: \\(p = .four places\\)"
  },
  {
    "objectID": "Lab-7.html#t.test",
    "href": "Lab-7.html#t.test",
    "title": "Lab 7",
    "section": "T.Test",
    "text": "T.Test\nThe t-test, as you will come to see, is quite similar to the z-test. The main differences are where the information is coming from.\nIf you remember, the z-test takes information from the population such as the mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThe t-test takes information from the sample instead.\nYou will notice the formulas are set-up the same way, they just have different numbers.\nZ-Test and T-Test Comparison\n\\(Z = \\frac{\\bar{X}-\\mu}{\\sigma_\\bar{X}}\\) and \\(t = \\frac{\\bar{X}-\\mu}{\\sigma_\\bar{X}}\\)\nTypes of T-Tests\nThere are three types of T-Tests, but we will only be learning one for today.\n\nT-Test for single samples\n\n\\(t = \\frac{\\bar{X}-\\mu}{\\sigma_\\bar{X}}\\)\n\nT-Test for paired samples\n\n\\(t = \\frac{\\bar{D}}{\\sigma_\\bar{D}}\\) - T-Test for independent samples\n\\(t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{S^2_1}{N_1}+\\frac{S^2_2}{N_2}}}\\)\nT-Test Example (R-Calculation)\nLet us start off with a sample of 20 scores:\n\n# To reproduce\nset.seed(123)\n# Set vector of reaction time\nReaction_Time = round(rnorm(20,550,10.5),digits=2)\n\n# Set mean to test against \nnull_mu = 670\n\n# Calculate t\nt.test(Reaction_Time,mu=null_mu)\n\n\n    One Sample t-test\n\ndata:  Reaction_Time\nt = -51.897, df = 19, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 670\n95 percent confidence interval:\n 546.7078 556.2672\nsample estimates:\nmean of x \n 551.4875 \n\n\nWriting our results\nAs we discussed in the previous lecture, we will need to make sure to report our findings in proper statistical formats.\nSpecifically, when we report the results of a t-test it should look like this:\n\\[t(19) = -51.897, p <.001\\]\nT-Test Example (Hand-Calculation)\n\\[t = \\frac{\\bar{X}-\\mu}{\\sigma_\\bar{X}}\\] Let us imagine we have 10 numbers:\n{111.66,117.50,104.82,146.03,134.50,106.52,115.17,114.40,129.36,119.00}\n\nsd(c(111.66,117.50,104.82,146.03,134.50,106.52,115.17,114.40,129.36,119.00))\n\n[1] 12.99123\n\n\nThese numbers have a standard deviation of 12.99.\nFirst, we will need to take the mean:\n\\[\\frac{111.66+117.50+104.82+146.03+134.50+106.52+115.17+114.40+129.36+119.00}{10} = 119.896\\]\nWe want to see if this mean is different from the population mean (\\(\\mu\\)), 126.42.\nWe will also need to solve for the standard error of the mean (\\(s_\\bar{X}\\))\nRemember that \\(s_\\bar{X} = \\frac{s}{\\sqrt{n}}\\)\n\\(s_\\bar{X} = \\frac{12.99}{\\sqrt{10}} = 4.107\\)\nOur new equation should look something like this:\n\\(t = \\frac{119.896-126.42}{4.107} = -1.588\\)\n\\(t = -1.588\\)\nYou’ll notice that when we did the t-test in R we were given a p-value. When you calculate the value by hand, there is no such luck! So, you will have to check for a critical value to see if the value you’ve calculated is past the rejection zone.\nLet’s make sure this works in R as well:\n\nx= c(111.66,117.50,104.82,146.03,134.50,106.52,115.17,114.40,129.36,119.00)\nt.test(x,mu=126.42)\n\n\n    One Sample t-test\n\ndata:  x\nt = -1.588, df = 9, p-value = 0.1467\nalternative hypothesis: true mean is not equal to 126.42\n95 percent confidence interval:\n 110.6026 129.1894\nsample estimates:\nmean of x \n  119.896"
  },
  {
    "objectID": "Lab-7.html#power",
    "href": "Lab-7.html#power",
    "title": "Lab 7",
    "section": "Power",
    "text": "Power\nWhen we talk about power in the context of an experiment, we are referring to the probability of detecting an effect if the effect exists in nature. Power is an important part of experimental design. One of its many uses is in the realm of grant proposals. Let’s imagine you want to run an experiment and you would also like a grant so you can fund the study. When you propose the study, the board that will be reviewing your claim will want to know if it is likely to work. If you can show them that your experiment has high power, you will need fewer participants to find the effect (if it exists), and will cost less money!\nThere are several ways to compute power, however, we will focus on the computer assisted method, using a program called G-Power. You will not be required to know how to use the program intricately, rather, you will just have to know how to interpret the output of the program as well as input one or two parameters."
  },
  {
    "objectID": "Lab-8.html",
    "href": "Lab-8.html",
    "title": "Lab 8",
    "section": "",
    "text": "The t-test for paired samples may also be referred to as the t-test for correlated groups. In both cases, the subjects are measured twice. This can be actualized in a pre-treatment, post-treatment design where participants are measured on some aspect, given a treatment, and then measured again. The \\(H_0\\) for these t-tests are that the mean difference is 0. The \\(H_1\\) then becomes that the difference is greater than 0 or less than 0.\nWhen we performed a sign test, we needed to supply R with the difference vector. When we do a paired t-test, we can provide both vectors and R will do the rest for us!\n\n\nFirst, create a distribution we can use for reproducibility using set.seed().\nExtract and compute the important information from the example you are given.\nCreate both of your vectors first, then combine them into a data frame. It is entirely possible to do all of this in one line, but syntax errors can run rampant when you have multiple arguments being passed at the same time!\nRecall that the arguments for t.test() when you are using a single sample will not work when you are working with paired samples.\nThe arguments are as follows:\n\ny1,y2 where y1 and y2 are both numeric vectors of the same length.\n\npaired = TRUE``\" This tells R that we are performing a paired t-test.\n\nvar.equal = TRUE This tells R that you are assuming the variances of both vectors are the same, by doing this you compute a certain kind of test. More on this later in the semester.\nMaking the full function look like this: t.test(y1, y2, paired = TRUE, var.equal = TRUE).\n\nWith this in mind, compute the t.test for the following example:\n\nA researcher is interested in whether students will perform better when they are allowed to use a computer to take notes compared to when they are only allowed to use pen and paper. He has 15 students take notes with pen and paper and then tests them. He then has them take notes with a computer and then tests them. Are the pen and paper test scores worse than the computer test scores? Compute the appropriate test for this design at an \\(\\alpha\\) level of .05.\n\nUse a seed of 3400 to create two samples of 15 students. Our prediction is that the students who use pen and paper to take notes will perform worse on the test compared to the students who use a computer to take notes. Choose means that will be appropriate for our prediction to be supported (without making the differences too…different!)\nCalculate the t value for the experiment and report whether or not there is a significance, and if so, what that significance means.\nRemember that when reporting to follow the proper format: \\[t(df) = t_{obt}, p =\\]\nAfter you have your answers, tell me your \\(t_{obt}\\) and we will create a vector to show the different values we get."
  },
  {
    "objectID": "Lab-9.html",
    "href": "Lab-9.html",
    "title": "Lab 9",
    "section": "",
    "text": "Before we dive into a new test, let us take a second to look back at the tests we have already covered and remember what those tests allowed us to do, and what the data for those tests looked like:\nZ-Test:\n\n\nWhat does it do?\n\nThe Z-test allows us to see if the scores from one population differ from another.\n\n\n\nWhat does our data look like?\n\nIn order perform a Z-test, we need to have large enough sample to work with (N>30) and we need to know \\(\\mu\\) and \\(\\sigma\\).\n\n\n\nT-Test\n\nWhat does it do?\n\nThe T-test allows us to compare a sample to :\n\n\n\\(\\mu\\) when it is given, and when \\(\\sigma\\) is not. (one sample)\nanother sample with paired values. (paired-samples)\nanother sample with uncorrelated values (independent samples)\n\n\n\n\nWhat does our data look like?\n\nIn order to perform a T-test, we need to have a sample that we are comparing to either, \\(\\mu\\), paired values, or a sample of (hopefully) equal length.\n\n\n\nHow is ANOVA different?\nHere is what t-test for independent samples might look like:\n\n# Create a control group with an n of 20, mean of 4.5 and sd of 2\ncontrol = rnorm(20,4.5,2)\n\n# Create an experimental group with an n of 20, mean of 6.8 and sd of 1\nexperimental = rnorm(20,6.8,1)\n\n# Combine into a dataframe\nt_test_ex <-data.frame(control, experimental)\n\nhead(t_test_ex)\n\n   control experimental\n1 6.571991     5.475319\n2 1.964870     6.511091\n3 4.911581     7.088624\n4 3.432214     6.912287\n5 5.946126     4.695407\n6 4.122698     7.110058\n\n\nYou will notice that there are two columns, one for control and one for experimental.\nWe we have yet to discuss what this experiment actually is trying to answer, but, we can tell from looking at the data that whatever experimental is—it’s mean is higher than control and there could be a significant difference.\nHere is what an ANOVA might look like:\n\n# Create a condition vector with 10 entries for control, treatment 1 and 2\nCondition = rep(c(\"Control\", \"Trt1\", \"Trt2\"), each = 10)\n\n# Create a scores vector with 30 entries, a mean of 18 and standard deviation of 10\nScores = rnorm(30, 18, 10)\n\nanova_example <- data.frame(Condition, Scores)\n\nanova_example\n\n   Condition    Scores\n1    Control 16.482095\n2    Control 19.232794\n3    Control 18.801771\n4    Control 22.288809\n5    Control 16.346052\n6    Control  5.683864\n7    Control 14.647004\n8    Control  9.259133\n9    Control 25.627955\n10   Control -4.825307\n11      Trt1 14.869841\n12      Trt1 23.166493\n13      Trt1 33.696984\n14      Trt1 21.527892\n15      Trt1 17.561906\n16      Trt1 40.196122\n17      Trt1 24.531040\n18      Trt1 21.554160\n19      Trt1 -4.224033\n20      Trt1 24.410534\n21      Trt2 21.092599\n22      Trt2 17.127126\n23      Trt2 38.345295\n24      Trt2 18.841889\n25      Trt2  4.254096\n26      Trt2 27.248526\n27      Trt2 10.044654\n28      Trt2 31.720120\n29      Trt2 33.735547\n30      Trt2 15.432810\n\n\nAs you can see, we still have two columns, but the main difference is that what used to be a condition two separate samples has become one single sample, separated by the various conditions.\nHerein lies the main difference between t-tests and ANOVAs: the amount of conditions (independent variables) that can be compared.\nt-tests can only compare an independent variable with two conditions/levels, while ANOVAs can compare an independent variable with more than two conditions/levels.\nThere are several notes to make when dealing with data in the format of an ANOVA.\n\nIt becomes essential that you understand the differences between an independent variable and a dependent variable.\nWhen you are dealing with 3 or more conditions, you need to convert them into factors if they are not already.\n\nFactors are a type of ordered data that tell R which subjects are in which condition.\n\nYou can check if a vector is a factor by running the is.factor() function.\n\n\n\nWe we will go through some examples to see what it looks like when R recognizes a vector as a factor and when it does not.\nThe following example shows a vector of conditions. However, R does not know that these are conditions until we tell it that they are factors. If we are unsure if R knows if a vector is a factor, we can test it using is.factor :\n\n# Create a sample vector \nexample <- c(\"Control\",\"Control\",\"Placebo\",\"Placebo\",\"Drug\",\"Drug\")\nis.factor(example)\n\n[1] FALSE\n\n\nNow that we know that R does not consider this vector to be a factor, we can manually tell R to make it a factor.\n\n# Create factor vector\nfactor(example)\n\n[1] Control Control Placebo Placebo Drug    Drug   \nLevels: Control Drug Placebo\n\n\nNow R, will recognize example as a factor.\nSimilar to lm, the way we conduct an anova is by using the aov function. The syntax for calculating an ANOVA is:\naov(DV ~ IV)\n\n\n\n\n\n\nNote: There are several methods to compute an ANOVA (including lm) and other packages.\n\n\n\n\n\n\n\n# Calculate an ANOVA on the above data\n# Save to a variable, aov.test.mod\naov.test.mod <- aov(anova_example$Scores ~ anova_example$Condition)\n\n# Calculate summary statistics and report\nsummary(aov.test.mod)\n\n                        Df Sum Sq Mean Sq F value Pr(>F)\nanova_example$Condition  2  365.3   182.7   1.637  0.213\nResiduals               27 3012.6   111.6               \n\n\nThe above calculation would be reported as such:\n\nWe observed no main effect of condition on scores, as evidenced by an analysis of variance, F(2,27), .39, p = ns."
  },
  {
    "objectID": "Links.html",
    "href": "Links.html",
    "title": "Resources",
    "section": "",
    "text": "R and R-Studio\nR is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\nAdditional R resources\n\nGoogle is great, Google your problem\n\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways.\nDanielle Navarro wrote a free Psych Stats textbook using R, it’s worth checking out (some of our textbook are based on Danielle’s)\nI am currently writing another stats textbook (incorporating some of the above). You can read it while it’s being made right here https://crumplab.github.io/statistics/, also check out the lab manual for more specific things about doing various stats in R (also in draft right now) https://crumplab.github.io/statisticsLab/\n\nDaniell Navarro recently made this website for introducing R, it’s great, check it out (also made using this R markdown process): http://compcogscisydney.org/psyr/\n\nCheck out my slightly older programming book that also introduces R https://crumplab.github.io/programmingforpsych/\n\nThis is the definitive guide for all things R Markdown (you will find this very useful as you get better at this skill): https://bookdown.org/yihui/rmarkdown/"
  },
  {
    "objectID": "Review-Sheet.html",
    "href": "Review-Sheet.html",
    "title": "Exam 1 Review Sheet",
    "section": "",
    "text": "Exam 1 R-Review Sheet\nFunctions and Arguments\nHere are some important functions and arguments to know:\nplot() - This is the function to graphically visualize your data. Within this function can be several arguments.\n\nx,y:The independent and dependent variable you are interested in\nmain =,xlab =, ylab =, sub =: Title, x-axis lable, y-axis label, and caption.\nxlim =,ylim =: These set parameters for the graph that will be drawn.\ncol = : Designates a desired color for the lines, and points that will be drawn.\npch =: Point character, assigns shape for points drawn.\npoints(): Plot additional points on an existing graph with pre-defined graphical parameters\n\ncor() - This function will return the correlation, or r, of an entire dataframe, or of specified variables.\nhist() - This function will create a histogram of a given variable, displaying its frequencies.\n\n\nbreaks = - This argument allows you to specify a bin width of your choice.\n\nlm() - This function allows you to create a linear model from which you can predict a variable, from another.\n\n\nthe ~ is used to delineate between the predicting variable and the predictor variable.\n\nFor example: lm(x~y) is different than lm(y~x).\nWhen using lm(), it is common to name the model something that makes sense + .mod..\n\n\n\nsummary() - This can be used on entire datasets to return the minimum, maximum, and median values.\n\nThis can also be used to expand a linear model to give you values such as the coefficient of correlation, slope and intercept.\nSyntax\nWhen using R-Studio, it is important that the math you do in the console follows the same rules as the math you would do on a calculator or a piece of paper.\nFor example:\nI have a dataset with a mean of 10, and a standard deviation of 7. What is the probability of having a score of 18?\nWe know the formula for this is the Z-Score formula: \\[z = \\frac{x - M}{\\sigma}\\] Depending on how you enter this data into R, you will get two different answers:\n\nwrong = 18-10/7\nwrong\n\n[1] 16.57143\n\nright = (18-10)/7\nright\n\n[1] 1.142857\n\n\n() are very important to R, and your answers will either be incorrect or will not be output correctly if the () are misplaced/misused. Depending on how difficult you want to make your calculations, keeping track of your () placement is very important.\nThe below equation will not render the correct answer:\n\nincorrect = (100-200*300/4)/(300-120^2/4)*(220-320^2/4)\ncorrect = (100-(200*300/4)/(300-((120^2)/4)*(220-((320^2)/4))))\n\nincorrect\n\n[1] -114594.5\n\ncorrect\n\n[1] 99.99984\n\ndiff = incorrect-correct\ndiff\n\n[1] -114694.5\n\n\nNot too bad, we were only off by one hundred and fourteen thousand six hundred ninety-four.\nMath Operators\n() matter, quite a lot. Additionally, here are the math operators you can use:\n+\n-\n*\n/\n^\nsqrt()"
  },
  {
    "objectID": "Review_Sheet_2.html",
    "href": "Review_Sheet_2.html",
    "title": "Exam 2 Review Sheet",
    "section": "",
    "text": "Review Sheet 2\nbarplot - This is the overall function for creating a bar graph. It has several functions that go along with it.\n\ncol - This is the same idea from the color argument used in a scatter plot, however, with bar plots you need to add a concatenation if you are looking to plot more than one bar (which you should!).\n\ncol=c(\"white\",\"red\",\"blue\")\n\nnames.arg - This is similar to xlab,however, we are usually going to be referring to each bar by a different name so you will need to add a concatenation.\n\nnames.arg=c(\"Experimental\",\"Control\")\n\n\nData Generation\nset.seed() - This function specifies a ‘seed’ of your choice. By choosing a ‘seed’ you are ensuring the ability of the data you produce to be replicable either for the use of a problem set, or for troubleshooting.\nsample - This function allows you to create a sample from an existing vector. It has two arguments.\n\nx - The vector that you will be sampling from. It is important that whatever you decide to sample from x cannot be larger than the length of x.\nn - This specifies how many items you want to sample from x.\n\nrnorm - This function returns whatever size you specify as a vector from the normal distribution.\n\nN - This is the size of the vector that you would like.\nmean - This is what you would like the mean of the data returned to be.\nsd - This is what you would like the standard deviation of the data returned to be.\n\nExample: rnorm(100, 120, 10)\nlibrary() - This function loads a package outside of the ones native to R. So far, the only package we will be using is BSDA.\nBSDA\nz.test() - This is the function you will use to perform a z test. It contains several arguments.\n\nx - This will be your vector that continues the data you will be comparing to \\(\\mu\\) and \\(\\sigma\\).\nsigma.x - This will be the population standard deviation, \\(\\sigma\\), which will usually be given to you in a question.\nmu - This will be the population mean, \\(\\mu\\), which will usually be given to you in a question.\nalternative - This argument specifies the directionality of the hypothesis you wish to test.\n\ntwo.sided - This argument specifies that you wish to run a 2-tailed test.\nlesser - This argument specified that you wish to run a 1-tailed test indicating that your sample is less than the population mean, \\(\\mu\\).\ngreater - This argument specified that you wish to run a 1-tailed test indicating that your sample is greater than the population mean, \\(\\mu\\).\n\n\nHere is what a full z.test function could look like:\nz.test(x, mu = 12, sigma.x = 3.12, alternative =\"two.sided\")\nSIGN.test() - This is the function you will use to perform a sign test. It contains several arguments.\n\nx - This will be your vector that continues the data that is the difference between the first group and second group.\nmd - This will rarely be given, but implied. The sign test will test against the \\(H_0\\), which states that no difference exists, so 0 should be the default value.\nalternative - This argument specifies the directionality of the hypothesis you wish to test.\n\ntwo.sided - This argument specifies that you wish to run a 2-tailed test.\nlesser - This argument specified that you wish to run a 1-tailed test indicating that your sample is less than the population mean, \\(\\mu\\).\ngreater - This argument specified that you wish to run a 1-tailed test indicating that your sample is greater than the population mean, \\(\\mu\\).\n\n\nHere is what a full sign test function could look like:\nSIGN.test(x, md = 0, alternative = \"two.sided\")\nT-Test: One-Sample, Paired, Independent\nt.test - This function contains several arguments, the results, as well as the type of test that is run will be determined on which you specify.\n\nx - This is a vector that you be comparing against the population mean, \\(\\mu\\).\ny1, y2 - These arguments specify two distinct vectors of the same length to be compared.\nmu - This will be the population mean, \\(\\mu\\), which will usually be given to you in a question.\nalternative - This argument specifies the directionality of the hypothesis you wish to test.\ntwo.sided - This argument specifies that you wish to run a 2-tailed test.\nlesser - This argument specified that you wish to run a 1-tailed test indicating that your sample is less than the population mean, \\(\\mu\\).\ngreater - This argument specified that you wish to run a 1-tailed test indicating that your sample is greater than the population mean, \\(\\mu\\).\nvar.equal = TRUE - This argument specifies that the variances are assumed to be equal.\npaired = TRUE - If specified, this will result in R assuming the sample(s) are to be paired.\n\nDetermining which Test will be run:\nSingle Sample T-Test\nt.test(x, mu = 23, alternative =\"lesser\")\nPaired Sample T-Test\nt.test(y1, y2, paired = TRUE, var.equal= TRUE, alternative = \"greater\")\nIndependent T-Test\nt.test(y1, y2, var.equal= TRUE, alternative = \"two.sided\")"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Test",
    "section": "",
    "text": "summary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "test.html#r-markdown",
    "href": "test.html#r-markdown",
    "title": "Test",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com."
  }
]